{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Feature Selection\n",
    "\n",
    "\n",
    "\n",
    "In this practical exercise, you will learn essential aspects of feature selection techniques using Python, specifically Recursive Feature Elimination (RFE), Chi-square, and Lasso. They will gain hands-on experience in implementing these techniques in a machine learning pipeline, along with Random Forest as the classifier. They'll learn how to use GridSearchCV for hyperparameter tuning, and cross-validation for model evaluation. The exercise will also provide insights on model performance by generating ROC AUC curves. This application-based learning will reinforce theoretical understanding and enable students to apply feature selection methods in real-world scenarios.\n",
    "\n",
    "Data used in this practical \n",
    "```\n",
    "'breast_cancer data set from scikit learn'\n",
    "``` \n",
    "\n",
    "The dataset used in this practical exercise is the Breast Cancer Wisconsin (Diagnostic) Dataset, which is available in the sklearn datasets module. This dataset contains characteristics derived from digitized images of fine-needle aspirate (FNA) of a breast mass.\n",
    "\n",
    "Key characteristics of the dataset:\n",
    "\n",
    "Number of Instances: 569\n",
    "Number of Attributes: 30 numeric, predictive attributes/features\n",
    "Target variable: Diagnosis (M = malignant, B = benign)\n",
    "The features in the dataset are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image, such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. Each of these features is represented ten times in the dataset (mean, standard error, and \"worst\" or largest) leading to 30 features.\n",
    "\n",
    "The objective with this dataset is to predict whether a tumor is malignant or benign based on these features. It's a commonly used dataset for binary classification problems in machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries:\n",
    "\n",
    "This section imports the necessary libraries used for data manipulation, model training, feature selection, and metrics calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data:\n",
    "\n",
    "This loads the breast cancer dataset from sklearn and assigns the independent variables to X and the dependent variable to y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Features:\n",
    "\n",
    "1. Load the data \n",
    "2. This step uses StandardScaler to normalize the features which is crucial before applying LassoCV as it is sensitive to the scale of the input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pima Indians Diabetes dataset\n",
    "data = pd.read_csv('diabetes.tsv', sep = \"\\t\")\n",
    "X = data.iloc[:, :-1] # Select all columns except last column (which is target)\n",
    "X = X.iloc[:, 2:X.shape[1]]\n",
    "\n",
    "y = data.iloc[:, -1].values   # Select the last column (target)\n",
    "X.describe()\n",
    "\n",
    "feature_names = X.columns \n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features for LASSO\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature selection methods\n",
    "feature_selectors = {\n",
    "    \"rfe\": RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    \"chi2\": SelectKBest(score_func=chi2, k=4),  # Select top 4 features; you can change this according to your need\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start with RFE\n",
    "print(\"Starting with Recursive Feature Elimination (RFE)...\")\n",
    "\n",
    "selector = feature_selectors[\"rfe\"]\n",
    "roc_aucs = []\n",
    "\n",
    "# Apply StratifiedKFold cross-validation for RFE\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Feature selection\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Classifier training and prediction\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_selected, y_train)\n",
    "    y_pred_prob = rf.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    # ROC AUC calculation and append to list\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "# Print the average ROC AUC score across the folds\n",
    "print(f\"RFE CV ROC AUC: {np.mean(roc_aucs)} ± {np.std(roc_aucs)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range=(0, 100))\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now move on to Chi-square\n",
    "print(\"Moving on to Chi-square Feature Selection...\")\n",
    "\n",
    "selector = feature_selectors[\"chi2\"]\n",
    "roc_aucs = []\n",
    "feature_scores = []\n",
    "\n",
    "# Apply StratifiedKFold cross-validation for Chi-square\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Feature selection\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Append the feature scores\n",
    "    feature_scores.append(selector.scores_)\n",
    "\n",
    "    # Classifier training and prediction\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_selected, y_train)\n",
    "    y_pred_prob = rf.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    # ROC AUC calculation and append to list\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "\n",
    "# Print the average ROC AUC score across the folds\n",
    "print(f\"Chi-square CV ROC AUC: {np.mean(roc_aucs)} ± {np.std(roc_aucs)}\\n\")\n",
    "\n",
    "# # Print the average ROC AUC score across the folds\n",
    "# print(f\"Chi-square CV ROC AUC: {np.mean(roc_aucs)} ± {np.std(roc_aucs)}\")\n",
    "\n",
    "# Calculate average feature scores across folds and plot them\n",
    "avg_scores = np.mean(feature_scores, axis=0)\n",
    "plt.bar(feature_names, avg_scores)\n",
    "plt.title(\"Feature Importances (Chi-square Scores)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Task\n",
    "\n",
    "1. Task 1 : For you the task is to run \"Lasso\" feature selection on the data set. \n",
    "    - Use RandomForestClassifier as classifier\n",
    "    - Which is the most important features. \n",
    "    - What is the model performance ? "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27504229a786ecc4bac7f6801848e44cb1a63648e4d666cbb9fdea5e9f215e02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
